{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14203a60-227e-481b-829e-e1c766c76bc9",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea3bd5-1a24-4cf1-8bc2-e816c057d0f6",
   "metadata": {},
   "source": [
    "# **Data Normalization Techniques**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfbc2d-b49a-480a-82af-be76fad1b598",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec6aac-8453-4b57-acc2-3b831984ef7c",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data normalization. This includes identifying compensation-related columns, applying normalization techniques, and visualizing the data distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db826cf-7557-4ccb-9c3e-2f73fa996b10",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e63e1d-316a-45b0-a40a-d05bc3aed15e",
   "metadata": {},
   "source": [
    "In this lab, you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc4a2e-8145-4f56-8916-e213f9ebc01d",
   "metadata": {},
   "source": [
    "- Identify duplicate rows and remove them.\n",
    "\n",
    "- Check and handle missing values in key columns.\n",
    "\n",
    "- Identify and normalize compensation-related columns.\n",
    "\n",
    "- Visualize the effect of normalization techniques on data distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ab283-6194-4223-9856-93c78a57eea1",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277168a-3e58-4583-a167-a5d214cfbb02",
   "metadata": {},
   "source": [
    "## Hands on Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8f035-4fef-4ecd-af2d-5be3dd92b6c2",
   "metadata": {},
   "source": [
    "#### Step 1: Install and Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a92e35-c620-46e3-b69d-62784d36005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bc9df-12f8-42a1-8a5e-1c2664c2db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc8019-d998-4846-b124-90595dc541a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3f861-1f8d-4507-ae81-f31cea56d467",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5416ad2-c0e7-4b07-b60e-4b77f04482b8",
   "metadata": {},
   "source": [
    "We use the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b426cc-aa4e-4b27-9aaf-1a9becc07d04",
   "metadata": {},
   "source": [
    "The functions below will download the dataset into your browser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475f4e8-8144-423d-a920-de8fda37bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to check if data is loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a51691-1397-44b4-ac53-fad59a946c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d28ff0-0e9a-4bfd-8e88-cc501b21d99f",
   "metadata": {},
   "source": [
    "### Section 1: Handling Duplicates\n",
    "##### Task 1: Identify and remove duplicate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109d159-84db-4868-b0d1-39c699ee118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 1: Identify and remove duplicate rows\n",
    "\n",
    "# Count duplicates\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
    "print(f\"Remaining duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a636d-06e1-49f4-b8cc-9c1a25f72b3e",
   "metadata": {},
   "source": [
    "### Section 2: Handling Missing Values\n",
    "##### Task 2: Identify missing values in `CodingActivities`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6b73c-0559-47af-82b4-9cb98880600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 2: Identify missing values in CodingActivities\n",
    "\n",
    "if 'CodingActivities' in df.columns:\n",
    "    missing_count = df['CodingActivities'].isnull().sum()\n",
    "    print(f\"Missing values in CodingActivities: {missing_count}\")\n",
    "    print(f\"Percentage missing: {(missing_count / len(df) * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"CodingActivities column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd7018-e030-4023-a6ae-8b88b61cc969",
   "metadata": {},
   "source": [
    "##### Task 3: Impute missing values in CodingActivities with forward-fill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bc3ef-2e4d-4037-b059-8139b2e403da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 3: Impute missing values in CodingActivities with forward-fill\n",
    "\n",
    "if 'CodingActivities' in df.columns:\n",
    "    original_missing = df['CodingActivities'].isnull().sum()\n",
    "    \n",
    "    # Forward fill\n",
    "    df['CodingActivities'] = df['CodingActivities'].fillna(method='ffill')\n",
    "    \n",
    "    # Check remaining missing\n",
    "    after_missing = df['CodingActivities'].isnull().sum()\n",
    "    \n",
    "    print(f\"Missing before forward-fill: {original_missing}\")\n",
    "    print(f\"Missing after forward-fill: {after_missing}\")\n",
    "    print(f\"Values imputed: {original_missing - after_missing}\")\n",
    "else:\n",
    "    print(\"CodingActivities column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dfe92-d350-440b-87b7-4b0aa2b1dcdd",
   "metadata": {},
   "source": [
    "**Note**:  Before normalizing ConvertedCompYearly, ensure that any missing values (NaN) in this column are handled appropriately. You can choose to either drop the rows containing NaN or replace the missing values with a suitable statistic (e.g., median or mean).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0aa65c-9b6a-4504-a0c7-5d9a9d0b0835",
   "metadata": {},
   "source": [
    "### Section 3: Normalizing Compensation Data\n",
    "##### Task 4: Identify compensation-related columns, such as ConvertedCompYearly.\n",
    "Normalization is commonly applied to compensation data to bring values within a comparable range. Here, you’ll identify ConvertedCompYearly or similar columns, which contain compensation information. This column will be used in the subsequent tasks for normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511a0f0-da8d-4831-9ca4-1fd6a1087f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 4: Identify compensation-related columns\n",
    "\n",
    "# Find compensation columns\n",
    "comp_columns = [col for col in df.columns if 'comp' in col.lower() or 'salary' in col.lower()]\n",
    "\n",
    "print(\"Compensation-related columns:\")\n",
    "for col in comp_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Focus on ConvertedCompYearly\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    print(f\"\\nConvertedCompYearly statistics:\")\n",
    "    print(df['ConvertedCompYearly'].describe())\n",
    "    \n",
    "    missing = df['ConvertedCompYearly'].isnull().sum()\n",
    "    print(f\"\\nMissing values: {missing} ({(missing/len(df)*100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b2199-3988-48bb-8b90-d39f120d9200",
   "metadata": {},
   "source": [
    "##### Task 5: Normalize ConvertedCompYearly using Min-Max Scaling.\n",
    "Min-Max Scaling brings all values in a column to a 0-1 range, making it useful for comparing data across different scales. Here, you will apply Min-Max normalization to the ConvertedCompYearly column, creating a new column ConvertedCompYearly_MinMax with normalized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aef7db-414a-48ab-912b-7a4a1a29c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 5: Normalize ConvertedCompYearly using Min-Max Scaling\n",
    "\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    # Drop NaN values for normalization\n",
    "    df_clean = df.dropna(subset=['ConvertedCompYearly'])\n",
    "    \n",
    "    # Min-Max Scaling: (x - min) / (max - min)\n",
    "    min_val = df_clean['ConvertedCompYearly'].min()\n",
    "    max_val = df_clean['ConvertedCompYearly'].max()\n",
    "    \n",
    "    df_clean['ConvertedCompYearly_MinMax'] = (df_clean['ConvertedCompYearly'] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    print(\"Min-Max Normalization applied:\")\n",
    "    print(f\"Original range: [{min_val}, {max_val}]\")\n",
    "    print(f\"Normalized range: [0, 1]\")\n",
    "    print(f\"\\nSample normalized values:\")\n",
    "    print(df_clean[['ConvertedCompYearly', 'ConvertedCompYearly_MinMax']].head())\n",
    "    \n",
    "    # Update the main dataframe\n",
    "    df = df_clean\n",
    "else:\n",
    "    print(\"ConvertedCompYearly column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd173253-f7f4-4859-8ce5-cbb09458d957",
   "metadata": {},
   "source": [
    "##### Task 6: Apply Z-score Normalization to `ConvertedCompYearly`.\n",
    "\n",
    "Z-score normalization standardizes values by converting them to a distribution with a mean of 0 and a standard deviation of 1. This method is helpful for datasets with a Gaussian (normal) distribution. Here, you’ll calculate Z-scores for the ConvertedCompYearly column, saving the results in a new column ConvertedCompYearly_Zscore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab86a85-98f7-4fc6-a854-467d4e558f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 6: Apply Z-score Normalization to ConvertedCompYearly\n",
    "\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    # Z-score: (x - mean) / std\n",
    "    mean_val = df['ConvertedCompYearly'].mean()\n",
    "    std_val = df['ConvertedCompYearly'].std()\n",
    "    \n",
    "    df['ConvertedCompYearly_Zscore'] = (df['ConvertedCompYearly'] - mean_val) / std_val\n",
    "    \n",
    "    print(\"Z-score Normalization applied:\")\n",
    "    print(f\"Mean: {mean_val:.2f}\")\n",
    "    print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "    print(f\"\\nZ-score distribution:\")\n",
    "    print(df['ConvertedCompYearly_Zscore'].describe())\n",
    "    print(f\"\\nSample Z-scores:\")\n",
    "    print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Zscore']].head())\n",
    "else:\n",
    "    print(\"ConvertedCompYearly column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f7e51-30f5-4544-99f1-3fe2a39e6159",
   "metadata": {},
   "source": [
    "### Section 4: Visualization of Normalized Data\n",
    "##### Task 7: Visualize the distribution of `ConvertedCompYearly`, `ConvertedCompYearly_Normalized`, and `ConvertedCompYearly_Zscore`\n",
    "\n",
    "Visualization helps you understand how normalization changes the data distribution. In this task, create histograms for the original ConvertedCompYearly, as well as its normalized versions (ConvertedCompYearly_MinMax and ConvertedCompYearly_Zscore). This will help you compare how each normalization technique affects the data range and distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a4298-7e5a-4ab1-b68d-0bbdce99b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 7: Visualize distributions\n",
    "\n",
    "if all(col in df.columns for col in ['ConvertedCompYearly', 'ConvertedCompYearly_MinMax', 'ConvertedCompYearly_Zscore']):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Original distribution\n",
    "    axes[0].hist(df['ConvertedCompYearly'], bins=50, edgecolor='black')\n",
    "    axes[0].set_title('Original ConvertedCompYearly Distribution')\n",
    "    axes[0].set_xlabel('Compensation')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Min-Max normalized distribution\n",
    "    axes[1].hist(df['ConvertedCompYearly_MinMax'], bins=50, edgecolor='black', color='green')\n",
    "    axes[1].set_title('Min-Max Normalized Distribution')\n",
    "    axes[1].set_xlabel('Normalized Compensation (0-1)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Z-score normalized distribution\n",
    "    axes[2].hist(df['ConvertedCompYearly_Zscore'], bins=50, edgecolor='black', color='orange')\n",
    "    axes[2].set_title('Z-score Normalized Distribution')\n",
    "    axes[2].set_xlabel('Z-score')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization complete! Compare the three distributions above.\")\n",
    "else:\n",
    "    print(\"Required columns not found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275a756-c307-4921-ad42-fd260ed14487",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d178018-4063-4616-8954-9c3a914564c1",
   "metadata": {},
   "source": [
    "In this lab, you practiced essential normalization techniques, including:\n",
    "\n",
    "- Identifying and handling duplicate rows.\n",
    "\n",
    "- Checking for and imputing missing values.\n",
    "\n",
    "- Applying Min-Max scaling and Z-score normalization to compensation data.\n",
    "\n",
    "- Visualizing the impact of normalization on data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb23433-c177-41ff-9cbb-e77cecca5937",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "5b2314eae200c68d20ee3204d822e6fad4f5845945b4895f383c3007af43740d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
