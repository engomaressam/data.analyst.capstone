{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8be15d8-fa8b-4b4f-94d7-39ed53321abd",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee706056-1bac-4cb0-820e-b9824ebdb12a",
   "metadata": {},
   "source": [
    "# **Data Wrangling Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc38be-5625-4c22-b622-dca1d20a2134",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03086336-ee63-4d6a-a1ce-d243006d7057",
   "metadata": {},
   "source": [
    "In this lab, you will perform data wrangling tasks to prepare raw data for analysis. Data wrangling involves cleaning, transforming, and organizing data into a structured format suitable for analysis. This lab focuses on tasks like identifying inconsistencies, encoding categorical variables, and feature transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223ac3e-4f39-44fa-a3fe-e1e0badf69ac",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cddaf13-6966-4047-aa87-daebb0727a7b",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268cb62-6e7c-4d79-be7c-d946a22c6923",
   "metadata": {},
   "source": [
    "- Identify and remove inconsistent data entries.\n",
    "\n",
    "- Encode categorical variables for analysis.\n",
    "\n",
    "- Handle missing values using multiple imputation strategies.\n",
    "\n",
    "- Apply feature scaling and transformation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b4baf-895a-4feb-9b7e-a15aedf316ad",
   "metadata": {},
   "source": [
    "#### Intsall the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73057dbb-7947-47cb-a232-3b73ed6a98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8c69b-ce80-4c51-bef1-fa344e72a840",
   "metadata": {},
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944eb65-42f7-4a97-bc2a-87f69456a1a4",
   "metadata": {},
   "source": [
    "#### Step 1: Import the necessary module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636b4c-6417-4cbc-a15d-da5bdab947e7",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0302abc-6d29-4f75-9023-8bfd18365412",
   "metadata": {},
   "source": [
    "<h5>1.1 Import necessary libraries and load the dataset.</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acfb07-c289-45fc-a4a3-20bcad583eed",
   "metadata": {},
   "source": [
    "Ensure the dataset is loaded correctly by displaying the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bbb3f-df8d-45ec-8d6e-8f695d9a4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Stack Overflow survey data\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7df306-0dd0-4566-a376-bad64fbf6455",
   "metadata": {},
   "source": [
    "#### 2. Explore the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b356e-a83e-478a-bd04-da95500e0304",
   "metadata": {},
   "source": [
    "<h5>2.1 Summarize the dataset by displaying the column data types, counts, and missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b368e2-4d3c-4382-9b1c-458be459ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Task 2.1: Summarize dataset\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Missing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False).head(10))\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cdd1b-44d7-40cf-ba9f-ee347f747832",
   "metadata": {},
   "source": [
    "<h5>2.2 Generate basic statistics for numerical columns.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527b643-6963-4a64-aab2-297894062fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Task 2.2: Generate basic statistics\n",
    "\n",
    "print(\"Basic Statistics for Numerical Columns:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca71359-8f5b-4f33-afe9-af011c2eae37",
   "metadata": {},
   "source": [
    "### 3. Identifying and Removing Inconsistencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d423d-4338-423a-ae66-40981cc7809d",
   "metadata": {},
   "source": [
    "<h5>3.1 Identify inconsistent or irrelevant entries in specific columns (e.g., Country).</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43fee0-2805-495c-93df-2f93fa2c35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Task 3.1: Identify inconsistent entries\n",
    "\n",
    "if 'Country' in df.columns:\n",
    "    print(\"Unique countries:\")\n",
    "    print(f\"Total unique countries: {df['Country'].nunique()}\")\n",
    "    print(\"\\nTop 10 countries:\")\n",
    "    print(df['Country'].value_counts().head(10))\n",
    "    \n",
    "    # Check for potential inconsistencies\n",
    "    print(\"\\nChecking for potential inconsistencies...\")\n",
    "    countries = df['Country'].value_counts()\n",
    "    print(f\"Countries with less than 5 responses: {(countries < 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ff9e2-d19e-4175-9997-907c09a94df0",
   "metadata": {},
   "source": [
    "<h5>3.2 Standardize entries in columns like Country or EdLevel by mapping inconsistent values to a consistent format.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a5909-90b5-4a2b-80cc-ca4427fb97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 3.2: Standardize entries\n",
    "\n",
    "# Example: Standardize EdLevel\n",
    "if 'EdLevel' in df.columns:\n",
    "    print(\"Original EdLevel values:\")\n",
    "    print(df['EdLevel'].value_counts().head())\n",
    "    \n",
    "    # Mapping for standardization (example)\n",
    "    edlevel_mapping = {\n",
    "        'Bachelor's degree (B.A., B.S., B.Eng., etc.)': 'Bachelor',\n",
    "        'Master's degree (M.A., M.S., M.Eng., MBA, etc.)': 'Master',\n",
    "        'Some college/university study without earning a degree': 'Some College'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping where applicable\n",
    "    # df['EdLevel_Standardized'] = df['EdLevel'].replace(edlevel_mapping)\n",
    "    \n",
    "    print(\"\\nEdLevel standardization mapping created (example shown)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7396f-8152-454d-b0d6-ee70eaba540d",
   "metadata": {},
   "source": [
    "### 4. Encoding Categorical Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e675dd9-b427-4c7a-92c2-b711a8ebb6a0",
   "metadata": {},
   "source": [
    "<h5>4.1 Encode the Employment column using one-hot encoding.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bded1e-c766-46f0-9015-88228947549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 4.1: Encode Employment column using one-hot encoding\n",
    "\n",
    "if 'Employment' in df.columns:\n",
    "    print(\"Original Employment values:\")\n",
    "    print(df['Employment'].value_counts())\n",
    "    \n",
    "    # One-hot encoding\n",
    "    employment_encoded = pd.get_dummies(df['Employment'], prefix='Employment')\n",
    "    \n",
    "    print(f\"\\nOne-hot encoded columns created: {employment_encoded.shape[1]}\")\n",
    "    print(\"Sample encoded columns:\")\n",
    "    print(employment_encoded.columns.tolist()[:5])\n",
    "    \n",
    "    # Optionally concatenate with original dataframe\n",
    "    # df = pd.concat([df, employment_encoded], axis=1)\n",
    "    \n",
    "    print(\"\\nOne-hot encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d75d2-209f-4c56-b14e-b2f8037e76d7",
   "metadata": {},
   "source": [
    "### 5. Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167edac3-f763-4689-89f9-c2b499fe6ceb",
   "metadata": {},
   "source": [
    "<h5>5.1 Identify columns with the highest number of missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b722d-ab02-4316-a64c-96fd10d57f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 5.1: Identify columns with highest missing values\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df) * 100)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "missing_summary = missing_summary.sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Top 10 columns with most missing values:\")\n",
    "print(missing_summary.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494af17-d786-412c-8fa5-93007c842385",
   "metadata": {},
   "source": [
    "<h5>5.2 Impute missing values in numerical columns (e.g., `ConvertedCompYearly`) with the mean or median.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dc8e5-d3d2-449d-ae4e-022bd8ee36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 5.2: Impute missing values in ConvertedCompYearly\n",
    "\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    missing_before = df['ConvertedCompYearly'].isnull().sum()\n",
    "    \n",
    "    # Use median for imputation (better for skewed salary data)\n",
    "    median_comp = df['ConvertedCompYearly'].median()\n",
    "    \n",
    "    df['ConvertedCompYearly'] = df['ConvertedCompYearly'].fillna(median_comp)\n",
    "    \n",
    "    missing_after = df['ConvertedCompYearly'].isnull().sum()\n",
    "    \n",
    "    print(f\"Missing before imputation: {missing_before}\")\n",
    "    print(f\"Median value used: ${median_comp:,.2f}\")\n",
    "    print(f\"Missing after imputation: {missing_after}\")\n",
    "    print(f\"Values imputed: {missing_before - missing_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa6dd5-e442-4042-8e0b-7b047c5271c7",
   "metadata": {},
   "source": [
    "<h5>5.3 Impute missing values in categorical columns (e.g., `RemoteWork`) with the most frequent value.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8de2f3-b9f2-4b54-8545-cfab7573a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 5.3: Impute missing values in RemoteWork with most frequent value\n",
    "\n",
    "if 'RemoteWork' in df.columns:\n",
    "    missing_before = df['RemoteWork'].isnull().sum()\n",
    "    \n",
    "    # Use mode (most frequent value)\n",
    "    most_frequent = df['RemoteWork'].mode()[0]\n",
    "    \n",
    "    df['RemoteWork'] = df['RemoteWork'].fillna(most_frequent)\n",
    "    \n",
    "    missing_after = df['RemoteWork'].isnull().sum()\n",
    "    \n",
    "    print(f\"Missing before imputation: {missing_before}\")\n",
    "    print(f\"Most frequent value: '{most_frequent}'\")\n",
    "    print(f\"Missing after imputation: {missing_after}\")\n",
    "    print(f\"Values imputed: {missing_before - missing_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df590266-80ce-41e6-aec8-0e535b770629",
   "metadata": {},
   "source": [
    "### 6. Feature Scaling and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac8b9d-3b1a-472f-b49a-d8d0299204dc",
   "metadata": {},
   "source": [
    "<h5>6.1 Apply Min-Max Scaling to normalize the `ConvertedCompYearly` column.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241cb998-7b86-412a-b45c-9463c1bbf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 6.1: Apply Min-Max Scaling to ConvertedCompYearly\n",
    "\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    # Min-Max Scaling\n",
    "    min_val = df['ConvertedCompYearly'].min()\n",
    "    max_val = df['ConvertedCompYearly'].max()\n",
    "    \n",
    "    df['ConvertedCompYearly_MinMax'] = (df['ConvertedCompYearly'] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    print(\"Min-Max Scaling applied:\")\n",
    "    print(f\"Original range: [${min_val:,.2f}, ${max_val:,.2f}]\")\n",
    "    print(f\"Normalized range: [0, 1]\")\n",
    "    print(\"\\nSample values:\")\n",
    "    print(df[['ConvertedCompYearly', 'ConvertedCompYearly_MinMax']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556a89e-8ab3-4eaa-9d75-ced79f89ed2c",
   "metadata": {},
   "source": [
    "<h5>6.2 Log-transform the ConvertedCompYearly column to reduce skewness.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523f80b-9bcd-4761-a6ad-4f4705687f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 6.2: Log-transform ConvertedCompYearly\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    # Apply log transformation (add 1 to avoid log(0))\n",
    "    df['ConvertedCompYearly_Log'] = np.log1p(df['ConvertedCompYearly'])\n",
    "    \n",
    "    print(\"Log transformation applied:\")\n",
    "    print(\"\\nOriginal vs Log-transformed:\")\n",
    "    print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Log']].describe())\n",
    "    \n",
    "    # Check skewness reduction\n",
    "    original_skew = df['ConvertedCompYearly'].skew()\n",
    "    log_skew = df['ConvertedCompYearly_Log'].skew()\n",
    "    \n",
    "    print(f\"\\nSkewness comparison:\")\n",
    "    print(f\"Original skewness: {original_skew:.4f}\")\n",
    "    print(f\"Log-transformed skewness: {log_skew:.4f}\")\n",
    "    print(f\"Skewness reduced by: {abs(original_skew - log_skew):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04a76c-224c-4182-9f4c-ff0ed5bac396",
   "metadata": {},
   "source": [
    "### 7. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd7cc2-4615-4c03-a2a9-ed6a30ef2556",
   "metadata": {},
   "source": [
    "<h5>7.1 Create a new column `ExperienceLevel` based on the `YearsCodePro` column:</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d111c-2aac-4f34-9995-5d9fc75b1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 7.1: Create ExperienceLevel based on YearsCodePro\n",
    "\n",
    "if 'YearsCodePro' in df.columns:\n",
    "    def categorize_experience(years):\n",
    "        if pd.isna(years):\n",
    "            return 'Unknown'\n",
    "        elif years < 2:\n",
    "            return 'Junior'\n",
    "        elif years < 5:\n",
    "            return 'Mid-Level'\n",
    "        elif years < 10:\n",
    "            return 'Senior'\n",
    "        else:\n",
    "            return 'Expert'\n",
    "    \n",
    "    df['ExperienceLevel'] = df['YearsCodePro'].apply(categorize_experience)\n",
    "    \n",
    "    print(\"ExperienceLevel created!\")\n",
    "    print(\"\\nDistribution:\")\n",
    "    print(df['ExperienceLevel'].value_counts())\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    print(df[['YearsCodePro', 'ExperienceLevel']].head(10))\n",
    "else:\n",
    "    print(\"YearsCodePro column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9db65-501b-4f0a-b514-5b77fe7172b1",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10613a56-101f-4d4e-8d03-08bb7c5f54bc",
   "metadata": {},
   "source": [
    "In this lab, you:\n",
    "\n",
    "- Explored the dataset to identify inconsistencies and missing values.\n",
    "\n",
    "- Encoded categorical variables for analysis.\n",
    "\n",
    "- Handled missing values using imputation techniques.\n",
    "\n",
    "- Normalized and transformed numerical data to prepare it for analysis.\n",
    "\n",
    "- Engineered a new feature to enhance data interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b9608-d5e2-4b31-a8ce-09231fafce5f",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "1e8e234f19fd098e27b0518a87f18de690e1c51f1d3263d5690927d19971251e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
