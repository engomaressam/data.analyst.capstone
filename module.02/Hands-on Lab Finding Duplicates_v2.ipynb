{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9b1ccb-1e17-4325-a1eb-bbf9d9b8879a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd50f2e-3f4a-4f32-b732-a1961f756561",
   "metadata": {},
   "source": [
    "# **Finding Duplicates Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860df127-9e5f-4b29-846e-8ba12d536784",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1784873-6ea2-46de-aab6-34c3c6fee8d6",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857c3f1-c604-4d6b-b551-19f4f356ee7e",
   "metadata": {},
   "source": [
    "Data wrangling is a critical step in preparing datasets for analysis, and handling duplicates plays a key role in ensuring data accuracy. In this lab, you will focus on identifying and removing duplicate entries from your dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ed7e0-619e-4a5c-8fab-7b74be9dda81",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fa756-8965-4cbf-a568-ad98362ae921",
   "metadata": {},
   "source": [
    "In this lab, you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b7a9a-5368-49f8-b2d2-8329f5e14fc0",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows in the dataset and analyze their characteristics.\n",
    "2. Visualize the distribution of duplicates based on key attributes.\n",
    "3. Remove duplicate values strategically based on specific criteria.\n",
    "4. Outline the process of verifying and documenting duplicate removal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8329849-34c2-4d9e-9567-15370e8fef6c",
   "metadata": {},
   "source": [
    "## Hands on Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7bdf8-2e11-44f5-b6e3-e9c2e7ad766c",
   "metadata": {},
   "source": [
    "Install the needed library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dc645-7958-4d22-a004-b18d62a6efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850c702-b3c6-472e-97e6-784dfc4d7657",
   "metadata": {},
   "source": [
    "Import pandas module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01f14b-8e35-4d16-8ca5-78088b3dc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b5037-750e-4f9c-a3c8-be19a0f0fb7f",
   "metadata": {},
   "source": [
    "Import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be1eb5-be4d-4a83-a184-979457428725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e8452-3c54-43e4-a568-19916e762331",
   "metadata": {},
   "source": [
    "## **Load the dataset into a dataframe**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0de7ff-2a28-43df-b9f0-a3329da2b27e",
   "metadata": {},
   "source": [
    "<h2>Read Data</h2>\n",
    "<p>\n",
    "We utilize the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2146c8-bbb3-4b87-ac0a-e15d099c688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset directly from the URL\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VYPrOu0Vs3I0hKLLjiPGrA/survey-data-with-duplicate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cfff6-094b-4d40-9eae-99d13b91a16f",
   "metadata": {},
   "source": [
    "Load the data into a pandas dataframe:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5c700-c03b-4417-b075-c2b8e1f468ea",
   "metadata": {},
   "source": [
    "Note: If you are working on a local Jupyter environment, you can use the URL directly in the pandas.read_csv() function as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2347e-fa12-425b-a7c7-52462add160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039095a6-af41-42a5-bc15-1f5715b30ad6",
   "metadata": {},
   "source": [
    "## Identify and Analyze Duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09621e9-02ae-46b9-88dd-0b8a96b1336b",
   "metadata": {},
   "source": [
    "### Task 1: Identify Duplicate Rows\n",
    "1. Count the number of duplicate rows in the dataset.\n",
    "3. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a5f12-97d0-4c18-a1a4-dce70f8418df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 1: Identify Duplicate Rows\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# Display the first few duplicate rows\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "print(f\"\\nTotal rows including all duplicates: {len(duplicates)}\")\n",
    "print(\"\\nFirst 5 duplicate rows:\")\n",
    "print(duplicates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d4b89-d51b-4889-bf2f-38fcb0a7b926",
   "metadata": {},
   "source": [
    "### Task 2: Analyze Characteristics of Duplicates\n",
    "1. Identify duplicate rows based on selected columns such as MainBranch, Employment, and RemoteWork. Analyse which columns frequently contain identical values within these duplicate rows.\n",
    "2. Analyse the characteristics of rows that are duplicates based on a subset of columns, such as MainBranch, Employment, and RemoteWork. Determine which columns frequently have identical values across these rows.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9928b3-ee07-45a2-8f88-cf339457c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 2: Analyze Characteristics of Duplicates\n",
    "\n",
    "# Identify duplicates based on selected columns\n",
    "subset_cols = ['MainBranch', 'Employment', 'RemoteWork']\n",
    "duplicates_subset = df[df.duplicated(subset=subset_cols, keep=False)]\n",
    "\n",
    "print(f\"Duplicate rows based on {subset_cols}: {len(duplicates_subset)}\")\n",
    "\n",
    "# Analyze which columns have identical values in duplicates\n",
    "print(\"\\nAnalyzing columns with identical values in duplicate rows:\")\n",
    "for col in df.columns:\n",
    "    if duplicates_subset[col].nunique() < len(duplicates_subset):\n",
    "        print(f\"- {col}: {duplicates_subset[col].nunique()} unique values in {len(duplicates_subset)} duplicate rows\")\n",
    "\n",
    "# Show value counts for the subset columns\n",
    "print(\"\\nValue distribution in duplicate rows:\")\n",
    "for col in subset_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(duplicates_subset[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805a48c-e8b3-42e3-8e67-8de52afc3f09",
   "metadata": {},
   "source": [
    "### Task 3: Visualize Duplicates Distribution\n",
    "1. Create visualizations to show the distribution of duplicates across different categories.\n",
    "2. Use bar charts or pie charts to represent the distribution of duplicates by Country and Employment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5016a8-583b-4db1-adb9-681b72aeba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 3: Visualize Duplicates Distribution\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Bar chart for duplicates by Country (top 10)\n",
    "if 'Country' in duplicates.columns:\n",
    "    country_counts = duplicates['Country'].value_counts().head(10)\n",
    "    axes[0, 0].bar(range(len(country_counts)), country_counts.values)\n",
    "    axes[0, 0].set_xticks(range(len(country_counts)))\n",
    "    axes[0, 0].set_xticklabels(country_counts.index, rotation=45, ha='right')\n",
    "    axes[0, 0].set_title('Top 10 Countries in Duplicate Rows')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Pie chart for duplicates by Employment\n",
    "if 'Employment' in duplicates.columns:\n",
    "    employment_counts = duplicates['Employment'].value_counts()\n",
    "    axes[0, 1].pie(employment_counts.values, labels=employment_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Employment Distribution in Duplicates')\n",
    "\n",
    "# 3. Bar chart for RemoteWork\n",
    "if 'RemoteWork' in duplicates.columns:\n",
    "    remote_counts = duplicates['RemoteWork'].value_counts()\n",
    "    axes[1, 0].bar(remote_counts.index, remote_counts.values)\n",
    "    axes[1, 0].set_title('Remote Work Distribution in Duplicates')\n",
    "    axes[1, 0].set_xlabel('Remote Work Status')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Bar chart for MainBranch\n",
    "if 'MainBranch' in duplicates.columns:\n",
    "    branch_counts = duplicates['MainBranch'].value_counts()\n",
    "    axes[1, 1].bar(range(len(branch_counts)), branch_counts.values)\n",
    "    axes[1, 1].set_xticks(range(len(branch_counts)))\n",
    "    axes[1, 1].set_xticklabels(branch_counts.index, rotation=45, ha='right')\n",
    "    axes[1, 1].set_title('Main Branch Distribution in Duplicates')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization complete: {num_duplicates} duplicate rows analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87211b7d-14f3-41b8-a883-9f77d25f55c1",
   "metadata": {},
   "source": [
    "### Task 4: Strategic Removal of Duplicates\n",
    "1. Decide which columns are critical for defining uniqueness in the dataset.\n",
    "2. Remove duplicates based on a subset of columns if complete row duplication is not a good criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c64b7-5b48-4837-b139-1bc410faa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 4: Strategic Removal of Duplicates\n",
    "\n",
    "# Define critical columns for uniqueness\n",
    "# ResponseId should be unique for each survey response\n",
    "critical_columns = ['ResponseId']\n",
    "\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "\n",
    "# Remove duplicates based on critical columns\n",
    "df_cleaned = df.drop_duplicates(subset=critical_columns, keep='first')\n",
    "\n",
    "print(\"Dataset shape after removing duplicates:\", df_cleaned.shape)\n",
    "print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} duplicate rows\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "remaining_duplicates = df_cleaned.duplicated(subset=critical_columns).sum()\n",
    "print(f\"\\nRemaining duplicates based on {critical_columns}: {remaining_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e36e50-897e-49ad-bbce-c788043a4424",
   "metadata": {},
   "source": [
    "## Verify and Document Duplicate Removal Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e47f9d-977b-420c-8c13-c050ea305392",
   "metadata": {},
   "source": [
    "### Task 5: Documentation\n",
    "1. Document the process of identifying and removing duplicates.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ac861d-f2ff-41bc-83ee-441a92281ba6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Write your explanation here\n",
    "\n",
    "\"\"\"\n",
    "DOCUMENTATION: Identifying and Removing Duplicates\n",
    "\n",
    "Process:\n",
    "1. Initial Identification: We identified duplicate rows using df.duplicated()\n",
    "   - Found duplicate rows in the dataset\n",
    "   - Examined both complete row duplicates and duplicates based on specific columns\n",
    "\n",
    "2. Analysis Phase: We analyzed the characteristics of duplicates\n",
    "   - Examined duplicates based on MainBranch, Employment, and RemoteWork columns\n",
    "   - Identified which columns frequently had identical values across duplicate rows\n",
    "   - This helped understand the nature and source of duplicates\n",
    "\n",
    "3. Visualization: Created multiple charts to show duplicate distribution\n",
    "   - Bar charts for Country and MainBranch distributions\n",
    "   - Pie chart for Employment status\n",
    "   - These visualizations helped identify patterns in duplicate data\n",
    "\n",
    "4. Strategic Removal: Used ResponseId as the critical column for uniqueness\n",
    "   - Each survey response should have a unique ResponseId\n",
    "   - Kept the first occurrence and removed subsequent duplicates\n",
    "   - This approach preserves the original survey responses while eliminating true duplicates\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fbbaa-0de2-492a-91d8-85111ef48c8b",
   "metadata": {},
   "source": [
    "2. Explain the reasoning behind selecting specific columns for identifying and removing duplicates.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5892c110-4910-4875-93dc-4463d69692ca",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Write your explanation here\n",
    "\n",
    "\"\"\"\n",
    "REASONING: Column Selection for Duplicate Removal\n",
    "\n",
    "Why ResponseId was chosen as the critical column:\n",
    "1. Unique Identifier: ResponseId is designed to be a unique identifier for each survey response\n",
    "   - Each respondent should have exactly one ResponseId\n",
    "   - Duplicate ResponseIds indicate data collection or processing errors\n",
    "\n",
    "2. Data Integrity: Using ResponseId ensures we maintain data quality\n",
    "   - Prevents counting the same person's response multiple times\n",
    "   - Preserves the statistical validity of the survey\n",
    "\n",
    "3. Alternative Approach: We could also consider subset duplicates based on:\n",
    "   - MainBranch + Employment + RemoteWork + Country + Age\n",
    "   - However, this might remove legitimate responses from different people with similar characteristics\n",
    "\n",
    "4. Keep='first' Strategy: We kept the first occurrence because:\n",
    "   - The first entry is likely the original, valid response\n",
    "   - Subsequent duplicates may be data entry errors or system glitches\n",
    "   - This is a conservative approach that preserves the earliest data point\n",
    "\n",
    "5. Business Context: In survey analysis:\n",
    "   - Duplicate responses can skew results and lead to incorrect conclusions\n",
    "   - Removing them ensures each respondent is represented only once\n",
    "   - This maintains the representativeness of the sample\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7e20a-1bf6-4527-8e96-72327c45cd80",
   "metadata": {},
   "source": [
    "### Summary and Next Steps\n",
    "**In this lab, you focused on identifying and analyzing duplicate rows within the dataset.**\n",
    "\n",
    "- You employed various techniques to explore the nature of duplicates and applied strategic methods for their removal.\n",
    "- For additional analysis, consider investigating the impact of duplicates on specific analyses and how their removal affects the results.\n",
    "- This version of the lab is more focused on duplicate analysis and handling, providing a structured approach to deal with duplicates in a dataset effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf91f9-2dda-45b4-9fdd-0bc91a1484d3",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11- 05|1.3|Madhusudhan Moole|Updated lab|\n",
    "|2024-10-28|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066efe23-175b-4c0f-8abc-379169aec311",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "fa3493caccf457f2b33a3a72ca6bf5789c2ce4157ea6e40534b09cc8380e8ae5"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
