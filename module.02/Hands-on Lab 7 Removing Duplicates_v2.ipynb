{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4089c30-998d-4931-94fb-3807af483a16",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a5554-9f50-4b1e-a632-ac5590183320",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfb6d2-3bd1-45ea-a33a-4d675cf11a72",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b3be6-51e3-426e-bb72-3c7c7b1c1e26",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8d372-40e9-4ee5-8746-d8c7e93b129a",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f21ccd-45c7-4041-93cb-7b9f429d09a8",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f1012-5ec8-484f-9d10-523bd4867839",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474916c-2f4f-473e-ae1f-e0801a99f9d0",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e249f1b-f8c6-4199-9ce2-949601a16f63",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce7f49-1def-492f-bf4e-9426905aa538",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194e879-7c33-4bb4-882c-d622d2976d25",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5c903-c3d0-4d84-a6fb-f4425e7fabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6dc40-2c53-4d73-a5f8-52612f10fa41",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536efa68-176b-46ad-b8a0-6e3ca04ded86",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634011a-4b1c-4011-97a5-05e9bc4307aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a6b07-9493-4222-a252-b5f307e00ceb",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3c359-1d38-45af-a85e-1fb5dc343cee",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980ed9c-9980-428a-96c8-a572e46dd842",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed006ac8-24ab-407c-95b5-43421768fb6d",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752e262-413b-4be3-8a79-0e55a3d6fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 1: Identify Duplicate Rows\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# Display the first few duplicate rows\n",
    "print(\"\\nFirst 5 duplicate rows:\")\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(duplicate_rows.head())\n",
    "\n",
    "# Show the shape before removal\n",
    "print(f\"\\nDataset shape before removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91c3ed-ff29-4262-a766-6b77e02471d7",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4860605-3bbb-483e-a8bb-a02ce11d0e51",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b0244-0b07-46aa-b20b-38a08389f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 2: Remove Duplicates\n",
    "\n",
    "# Remove duplicate rows using drop_duplicates()\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "print(f\"Dataset shape after removing duplicates: {df_cleaned.shape}\")\n",
    "print(f\"Number of rows removed: {df.shape[0] - df_cleaned.shape[0]}\")\n",
    "\n",
    "# Verify the removal by counting duplicates again\n",
    "remaining_duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"\\nRemaining duplicate rows: {remaining_duplicates}\")\n",
    "\n",
    "# Update df to use the cleaned version\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0bdc3-67b8-4abd-b25e-815c7e158854",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272e513-db29-42a8-9c89-a6418fdbd460",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e8b4d-ab9b-4a95-bb3b-2d579fb58a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 3: Identify and Handle Missing Values\n",
    "\n",
    "# Identify missing values for all columns\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0].sort_values(ascending=False))\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "missing_percent = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_percent[missing_percent > 0].head(10))\n",
    "\n",
    "# Choose EdLevel column for imputation\n",
    "if 'EdLevel' in df.columns:\n",
    "    print(f\"\\nMissing values in EdLevel: {df['EdLevel'].isnull().sum()}\")\n",
    "    \n",
    "    # Find the most frequent value\n",
    "    most_frequent_edlevel = df['EdLevel'].mode()[0]\n",
    "    print(f\"Most frequent value in EdLevel: {most_frequent_edlevel}\")\n",
    "    \n",
    "    # Impute missing values with the most frequent value\n",
    "    df['EdLevel'].fillna(most_frequent_edlevel, inplace=True)\n",
    "    \n",
    "    # Verify imputation\n",
    "    print(f\"Missing values in EdLevel after imputation: {df['EdLevel'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976ce70-f8fb-48ac-9688-fc543643e758",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc16e21-275b-41bb-ab92-1a24c7350cec",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68806f-0578-42e2-a60d-decce6b2c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Task 4: Normalize Compensation Data Using ConvertedCompYearly\n",
    "\n",
    "# Check if ConvertedCompYearly column exists\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    print(\"ConvertedCompYearly column statistics:\")\n",
    "    print(df['ConvertedCompYearly'].describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_comp = df['ConvertedCompYearly'].isnull().sum()\n",
    "    print(f\"\\nMissing values in ConvertedCompYearly: {missing_comp}\")\n",
    "    print(f\"Percentage of missing values: {(missing_comp / len(df) * 100):.2f}%\")\n",
    "    \n",
    "    # Handle missing values - we can use median imputation for compensation\n",
    "    if missing_comp > 0:\n",
    "        median_comp = df['ConvertedCompYearly'].median()\n",
    "        print(f\"\\nMedian compensation: ${median_comp:,.2f}\")\n",
    "        \n",
    "        df['ConvertedCompYearly'].fillna(median_comp, inplace=True)\n",
    "        print(f\"Missing values after imputation: {df['ConvertedCompYearly'].isnull().sum()}\")\n",
    "    \n",
    "    # Display sample of normalized compensation data\n",
    "    print(\"\\nSample of ConvertedCompYearly (normalized annual compensation):\")\n",
    "    print(df[['ConvertedCompYearly']].head(10))\n",
    "else:\n",
    "    print(\"ConvertedCompYearly column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1f16c-8a82-4fd6-8179-0f7c1af2bc65",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8553f-648c-4969-b7f7-184153b2ed66",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa13dd-2138-408d-bd7c-921ebd1836ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Summary of data wrangling steps performed\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA WRANGLING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Duplicate Removal:\")\n",
    "print(f\"   - Identified and removed duplicate rows\")\n",
    "print(f\"   - Final dataset shape: {df.shape}\")\n",
    "\n",
    "print(f\"\\n2. Missing Value Handling:\")\n",
    "print(f\"   - Imputed EdLevel with most frequent value\")\n",
    "print(f\"   - Imputed ConvertedCompYearly with median value\")\n",
    "\n",
    "print(f\"\\n3. Data Normalization:\")\n",
    "print(f\"   - Used ConvertedCompYearly for standardized compensation analysis\")\n",
    "\n",
    "print(f\"\\n4. Final Data Quality:\")\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"   - Total missing values remaining: {total_missing}\")\n",
    "print(f\"   - Dataset is ready for analysis!\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4bc62-6ea9-4cd9-9609-a3c54756ea1a",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f536a0-5027-4054-a483-45893b9847ce",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
